在对源码进行初步研究后，发现系统自带的分词不能如StandardAnalyzer等对中文不能有良好的分词效果，分出来的都是一个字一个字的，这样达不到分词
效果，因此打算采用之前最新的分词系统来嵌入到Lucene.neyt中。
在往网上看到了一些Lucene.Net的分词，但是问题有：
1 Lucene.Net版本陈旧；
2 分词系统版本陈旧；
3 很多都是基于java的，不能直接移植到.net中；
研究参考的网站有：
http://dev.yesky.com/msdn/344/7578844_2.shtml
http://manan.org/2011/08/using_ictclas_in_java_and_lucene/
http://hellotommy.iteye.com/blog/826333
http://blog.csdn.net/chaocy/article/details/5938741
http://dev.yesky.com/msdn/344/7578844_2.shtml

在此基础上实现了一个基于控制台的分词程序，测试字符串
“我是一个中国人，我热爱自己的国家 I'll love you forever gouwa@yeah.net 2014 192.168 ! ber”

结果如下所示：
我      0       1       1
是      2       3       1
一个    4       6       1
中国    7       9       1
人      10      11      1
，      12      13      1
我      14      15      1
热爱    16      18      1
自己    19      21      1
的      22      23      1
国家    24      26      1
i       29      30      1
'       31      32      1
ll      33      35      1
love    38      42      1
you     45      48      1
forev   51      58      1
gouwa@yeah.net  61      75      1
2014    78      82      1
192.168 85      92      1
!       95      96      1
ber     99      102     1
请按任意键继续. . .
